version: '2.3'

x-airflow: &airflow
  build:
    context: .
    target: dev
  environment:
    - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    - AIRFLOW__CELERY__WORKER_AUTOSCALE=16,12
    - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/1
    - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${META_USER}:${META_PASS}@metadb:5432/airflow
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${META_USER}:${META_PASS}@metadb:5432/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}

services:

  redis:
   image: redis:5.0.5
   healthcheck:
    test: ["CMD", "redis-cli", "ping"]
    interval: 1s
    timeout: 3s
    retries: 30

  pg:
    image: postgres:13.4-alpine
    environment:
      - POSTGRES_USER=${META_USER}
      - POSTGRES_PASSWORD=${META_PASS}
    ports:
      - "5004:5432"
    volumes:
      - ./docker/init-pg.sql:/docker-entrypoint-initdb.d/init-pg.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 10s
      timeout: 5s
      retries: 50


  metadb:
    image: postgres:13.4-alpine
    environment:
      - POSTGRES_USER=${META_USER}
      - POSTGRES_PASSWORD=${META_PASS}
    ports:
      - "5005:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 2s
      timeout: 5s
      retries: 10
    restart: always


  sftp_service:
    build:
      context: docker/sftp_service
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 22"]
      interval: 5s
      timeout: 5s
      retries: 10


#  web_service:
#    build:
#      context: docker/web_service
#    ports:
#      - "5656:5000"
#    volumes:
#      -  ./docker/web_service:/usr/src/app


  s3work:
    image: localstack/localstack:0.11.5
    ports:
      - 4566:4566
      - 8055:8080
    environment:
      - SERVICES=s3
      - DEBUG=0
      - DATA_DIR=/tmp/localstack/data
    volumes:
      - ./docker/init-s3.sh:/docker-entrypoint-initaws.d/init-s3.sh
    healthcheck:
      test: bash -c 'AWS_ACCESS_KEY_ID=fake AWS_SECRET_ACCESS_KEY=fake aws --endpoint-url=http://localhost:4566 s3 ls'
      interval: 3s
      timeout: 10s
      retries: 10
    restart: always


  scheduler:
    <<: *airflow
    depends_on:
      metadb:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./common:${AIRFLOW_HOME}/common
      - ./dags:${AIRFLOW_HOME}/dags
      - ./tests:${AIRFLOW_HOME}/tests
      - ./tests_data:${AIRFLOW_HOME}/tests_data
      - ./conftest.py:${AIRFLOW_HOME}/conftest.py
    ports:
      - 127.0.0.1:9922:22
    command: scheduler-ssh
    restart: always

  webserver:
    <<: *airflow
    depends_on:
      - scheduler
    volumes:
      - ./dags/:${AIRFLOW_HOME}/dags
      - ./common/:${AIRFLOW_HOME}/common
    ports:
      - 8080:8080
    command: webserver
    healthcheck:
      test: curl -s -I -o /dev/null -w '%{http_code}' webserver:8080 | grep -qE '302|200'
      interval: 30s
      timeout: 30s
      retries: 3
    restart: always


  flower:
    <<: *airflow
    depends_on:
      - scheduler
    ports:
      - 5555:5555
    command: flower
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://flower:5555/"]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always


  worker:
    <<: *airflow
    scale: 1
    depends_on:
      - scheduler
    volumes:
      - ./common/:${AIRFLOW_HOME}/common
      - ./dags/:${AIRFLOW_HOME}/dags
    command: worker
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
