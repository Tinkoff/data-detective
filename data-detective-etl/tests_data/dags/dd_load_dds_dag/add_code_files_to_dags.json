{
    "schema":{
        "fields":[
            {
                "name":"dag_dir",
                "type":"string"
            },
            {
                "name":"dag_id",
                "type":"string"
            },
            {
                "name":"default_args",
                "type":"string"
            },
            {
                "name":"description",
                "type":"string"
            },
            {
                "name":"factory",
                "type":"string"
            },
            {
                "name":"meta_yaml",
                "type":"string"
            },
            {
                "name":"py_files",
                "type":"string"
            },
            {
                "name":"schedule_interval",
                "type":"string"
            },
            {
                "name":"sql_files",
                "type":"string"
            },
            {
                "name":"tags",
                "type":"string"
            },
            {
                "name":"tasks",
                "type":"string"
            },
            {
                "name":"yaml_files",
                "type":"string"
            }
        ],
        "pandas_version":"0.20.0"
    },
    "data":[
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_system_remove_works",
            "dag_id":"dd_system_remove_works",
            "default_args":{
                "owner":"airflow",
                "retries":1
            },
            "description":"Clean of unremoved works",
            "factory":"Python",
            "meta_yaml":"description: Clean of unremoved works\nschedule_interval: '5 3 * * *'\ntags:\n  - system\ndefault_args:\n  owner: airflow\n  retries: 1\nfactory: Python\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"import shutil\nimport tempfile\nfrom pathlib import Path\n\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.models.dagrun import DagRun\nfrom airflow.operators.python import PythonOperator\n\nfrom data_detective_airflow.constants import WORK_S3_PREFIX, WORK_PG_SCHEMA_PREFIX, WORK_FILE_PREFIX, WORK_S3_BUCKET, PG_CONN_ID\nfrom data_detective_airflow.dag_generator import TDag\n\nfrom common.constants import S3_WORK_ID\n\n\ndef get_active_run_ids():\n    return [str(run.id) for run in DagRun.find(state='running')]\n\n\ndef clean_s3_works(conn_id):\n    hook = S3Hook(conn_id)\n\n    active_run_ids = get_active_run_ids()\n    print('Active run ids:')\n    print(*active_run_ids, sep='\\n')\n\n    bucket_name = WORK_S3_BUCKET\n    prefix = f'{WORK_S3_BUCKET}\/{WORK_S3_PREFIX}'\n\n    bucket = hook.get_bucket(bucket_name)\n    for obj in bucket.objects.filter(Prefix=prefix):\n        dirname = obj.key.split('\/')[0]\n        run_id = dirname.split('_')[-1]\n\n        print(f'Trying to delete {obj.key}')\n        print(f'Dirname: {dirname}')\n        print(f'Run id from obj.key: {run_id}')\n\n        if run_id not in active_run_ids:\n            obj.delete()\n            print(f'{obj.key} deleted successfully')\n        else:\n            print(f'{obj.key} is in use now. Cant delete.')\n\n\ndef clean_pg_works(conn_id):\n    hook = PostgresHook(conn_id)\n\n    active_run_ids = get_active_run_ids()\n\n    sql_schemas = \"select nspname \"\\\n                  \"from pg_catalog.pg_namespace \"\\\n                  f\"where nspname like '{WORK_PG_SCHEMA_PREFIX}_%';\"\n\n    schemas = hook.get_pandas_df(sql_schemas)\n\n    schemas['active'] = schemas['nspname'].apply(\n        lambda nspname: nspname.split('_')[-1] in active_run_ids\n    )\n    schemas = schemas[~schemas['active']]\n\n    if not schemas.empty:\n        schemas['drop_sql'] = schemas['nspname'].apply(\n            lambda nspname: f'DROP SCHEMA IF EXISTS {nspname} CASCADE;')\n        hook.run(sql=schemas['drop_sql'].to_list(), autocommit=True)\n\n\ndef clean_local_works():\n    tmp_path = Path(tempfile.gettempdir())\n\n    active_run_ids = get_active_run_ids()\n\n    for path_ in tmp_path.iterdir():\n        if path_.is_dir() and path_.name.startswith(WORK_FILE_PREFIX):\n            run_id = path_.as_posix().split('_')[-1]\n            if run_id not in active_run_ids:\n                shutil.rmtree(path_)\n\n\ndef fill_dag(tdag: TDag):\n\n    PythonOperator(\n        task_id='clean_s3_works',\n        python_callable=clean_s3_works,\n        op_args=[S3_WORK_ID],\n        dag=tdag)\n\n    PythonOperator(\n        task_id='clean_pg_works',\n        python_callable=clean_pg_works,\n        op_args=[PG_CONN_ID],\n        dag=tdag)\n\n    PythonOperator(\n        task_id='clean_local_works',\n        python_callable=clean_local_works,\n        dag=tdag)\n"
                }
            ],
            "schedule_interval":"5 3 * * *",
            "sql_files":[

            ],
            "tags":[
                "system"
            ],
            "tasks":[
                "clean_s3_works",
                "clean_pg_works",
                "clean_local_works"
            ],
            "yaml_files":[

            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_dds_dag",
            "dag_id":"dd_load_dds_dag",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_conn_id":"s3work",
                "work_type":"s3"
            },
            "description":"Loading meta information from dag to data detective entity",
            "factory":"Python",
            "meta_yaml":"description: Loading meta information from dag to data detective entity\nschedule_interval: 18 03 * * *\ntags:\n  - dds\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_conn_id: s3work\n  work_type: s3\nfactory: Python\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"from pathlib import Path\n\nfrom airflow.models import DagBag\nfrom pandas import DataFrame\nimport petl\n\nfrom data_detective_airflow.dag_generator import TDag\nfrom data_detective_airflow.operators import PgSingleTargetLoader, PyTransform\n\nfrom common.builders import JsonSystemBuilder, CodeBuilder\nfrom common.utilities.entity_enums import (\n    ENTITY_CORE_FIELDS, RELATION_CORE_FIELDS,\n    EntityTypes, EntityFields,\n    RelationTypes, RelationFields,\n)\nfrom common.urn import get_etl_job, get_tree_node\nfrom common.utilities.search_enums import SystemForSearch, TypeForSearch\n\n\ndef _get_code_files(path: str, code_type: str, exclude_files: list = None) -> list[dict]:\n    \"\"\"Get contents of DAG code files\n\n    :param path: str name of absolute path to get code files\n    :param code_type: str suffix for code files (like *.py, *.sql, *.yaml)\n    :param exclude_files: list of relative path to exclude files\n    :return: list of dict [{'name': 'relative path to file name\n                            'type': 'suffix from code_type'\n                            'data': 'contents of file in text mode':}]\n    \"\"\"\n    pathname = Path(path)\n    if not exclude_files:\n        exclude_files = []\n\n    result = [{'name': str(code_file.relative_to(pathname)),\n               'type': code_type,\n               'data': code_file.read_text(encoding='utf-8')}\n              for code_file in pathname.glob(f'**\/*.{code_type}')\n              if code_file not in [pathname \/ ex_file for ex_file in exclude_files]]\n\n    return result\n\n\ndef get_list_of_dags(_context: dict) -> tuple[tuple]:\n    \"\"\"Get list of dags from airflow.models DagBag\n\n    :param _context: airflow DAG task run context\n    :return: tuple_of_tuples dags metadata ('dag_id', 'dag_dir', 'factory', 'schedule_interval', 'description',\n                                            'default_args', 'tags', 'tasks')\n    \"\"\"\n    dag_info = ['dag_id', 'dag_dir', 'factory', 'schedule_interval', 'description', 'default_args', 'tags']\n\n    dag_list = [dag_info + ['tasks']] + [\n        ([getattr(dag, info, None) for info in dag_info] + [[task.task_id for task in dag.tasks]])\n        for dag in DagBag().dags.values()\n    ]\n\n    return petl.wrap(dag_list).tupleoftuples()\n\n\ndef add_code_files_to_dags(_context: dict, dags: tuple[tuple]) -> tuple[tuple]:\n    \"\"\"Add code file contents to dags info\n\n    :param _context: airflow DAG task run context\n    :param dags: tuple_of_tuples dags metadata ('dag_id', 'dag_dir', 'factory', 'schedule_interval', 'description',\n                                                'default_args', 'tags', 'tasks')\n    :return: dags + ('meta_yaml', 'yaml_files', 'py_files', 'sql_files')\n    \"\"\"\n    result = (petl.wrap(dags)\n              .addfield('meta_yaml', lambda row: (Path(row['dag_dir']) \/ 'meta.yaml').read_text(encoding='utf-8'))\n              .addfield('yaml_files', lambda row: _get_code_files(row['dag_dir'], 'yaml', exclude_files=['meta.yaml']))\n              .addfield('py_files', lambda row: _get_code_files(row['dag_dir'], 'py'))\n              .addfield('sql_files', lambda row: _get_code_files(row['dag_dir'], 'sql'))\n              )\n\n    return result.tupleoftuples()\n\n\ndef transform_dag_to_entity(_context: dict, dags: tuple[tuple]) -> DataFrame:\n    \"\"\"Transform DAGs metadata to dds entity table format\n\n    :param _context: airflow DAG task run context\n    :param dags: dags metadata ('dag_id', 'dag_dir', 'factory', 'schedule_interval', 'description',\n                                'default_args', 'tags', 'tasks', 'meta_yaml', 'yaml_files', 'py_files', 'sql_files')\n    :return: [ENTITY_CORE_FIELDS] + [EntityFields.JSON_SYSTEM, EntityFields.INFO, EntityFields.CODES]\n    \"\"\"\n    json_system_builder = JsonSystemBuilder(\n        system_for_search=SystemForSearch.DATA_DETECTIVE.name,\n        type_for_search=TypeForSearch.JOB.name,\n    )\n\n    meta_yaml_code_builder = CodeBuilder(header='DAG main meta.yaml', opened=True, language='yaml')\n    dag_code_builder = CodeBuilder(opened=False)\n\n    result = (petl.wrap(dags)\n              .addfield(EntityFields.URN, lambda row: get_etl_job('dd', 'airflow', row['dag_id']))\n              .addfield(EntityFields.ENTITY_NAME, lambda row: row['dag_id'])\n              .addfield(EntityFields.ENTITY_NAME_SHORT, None)\n              .addfield(EntityFields.ENTITY_TYPE, EntityTypes.JOB)\n              .addfield(EntityFields.JSON_SYSTEM, json_system_builder())\n              .addfield(EntityFields.SEARCH_DATA,\n                        lambda row: f\"{row[EntityFields.URN]} {row[EntityFields.ENTITY_NAME]}\")\n              .addfield(EntityFields.JSON_DATA, lambda row: dict(factory=row['factory'],\n                                                                 default_args=row['default_args'],\n                                                                 schedule_interval=row['schedule_interval'],\n                                                                 tasks=row['tasks'],\n                                                                 meta_yaml=row['meta_yaml'],\n                                                                 tags=row['tags']))\n              .addfield(EntityFields.CODES,\n                        lambda row: (\n                            [meta_yaml_code_builder(data=row['meta_yaml'])] +\n                            [dag_code_builder(header=code['name'], language=code['type'], data=code['data'])\n                             for code in row['yaml_files']] +\n                            [dag_code_builder(header=code['name'], language=code['type'], data=code['data'])\n                             for code in row['py_files']] +\n                            [dag_code_builder(header=code['name'], language=code['type'], data=code['data'])\n                             for code in row['sql_files']]\n                        )\n                        )\n              .rename('description', EntityFields.INFO)\n              .rename('tags', EntityFields.TAGS)\n              # TAGS removed because of bag in dd-airflow JSON_FIELDS, need to add after dd-airflow release\n              .cut(list(ENTITY_CORE_FIELDS) + [EntityFields.JSON_SYSTEM, EntityFields.INFO, EntityFields.CODES])\n              .distinct(key=EntityFields.URN)\n              )\n\n    return result.todataframe()\n\n\ndef link_root_node_to_dag(_context: dict, dags: DataFrame) -> DataFrame:\n    \"\"\"Link dags to root tree node urn:tree_node:root:etl_dags\n\n    :param _context: airflow DAG task run context\n    :param dags: [ENTITY_CORE_FIELDS] + [EntityFields.JSON_SYSTEM, EntityFields.INFO, EntityFields.CODES]]\n    :return: DataFrame RELATION_CORE_FIELDS\n    \"\"\"\n    result = (petl.fromdataframe(dags)\n              .cut([EntityFields.URN])\n              .addfield(RelationFields.SOURCE, lambda row: get_tree_node(['ETL DAGS']))\n              .rename(EntityFields.URN, RelationFields.DESTINATION)\n              .addfield(RelationFields.TYPE, RelationTypes.Contains)\n              .addfield(RelationFields.ATTRIBUTE, None)\n              .cut(list(RELATION_CORE_FIELDS))\n              .distinct()\n              )\n    return result.todataframe()\n\n\ndef fill_dag(t_dag: TDag):\n\n    PyTransform(\n        task_id='get_list_of_dags',\n        description='Get list of DAGs from airflow DagBag',\n        transformer_callable=get_list_of_dags,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='add_code_files_to_dags',\n        description='Add code file contents to dags info',\n        source=['get_list_of_dags'],\n        transformer_callable=add_code_files_to_dags,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='transform_dag_to_entity',\n        description='Transform dags metadata to dds.entity',\n        source=['add_code_files_to_dags'],\n        transformer_callable=transform_dag_to_entity,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='link_root_node_to_dag',\n        description='Link dags to root tree node',\n        transformer_callable=link_root_node_to_dag,\n        source=['transform_dag_to_entity'],\n        dag=t_dag\n    )\n\n    PgSingleTargetLoader.upload_dds_entity(dag=t_dag, sources=['transform_dag_to_entity'])\n    PgSingleTargetLoader.upload_dds_relation(dag=t_dag, sources=['link_root_node_to_dag'])\n"
                }
            ],
            "schedule_interval":"18 03 * * *",
            "sql_files":[

            ],
            "tags":[
                "dds"
            ],
            "tasks":[
                "get_list_of_dags",
                "add_code_files_to_dags",
                "transform_dag_to_entity",
                "link_root_node_to_dag",
                "upload_dds_entity",
                "upload_dds_relation"
            ],
            "yaml_files":[

            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_dds_pg",
            "dag_id":"dd_load_dds_pg",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_conn_id":"s3work",
                "work_type":"s3"
            },
            "description":"Loading meta information from postgres database",
            "factory":"Python",
            "meta_yaml":"description: Loading meta information from postgres database\nschedule_interval: 13 03 * * *\ntags:\n  - dds\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_conn_id: s3work\n  work_type: s3\nfactory: Python\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"from pandas import DataFrame\nimport petl\n\nfrom data_detective_airflow.constants import PG_CONN_ID\nfrom data_detective_airflow.dag_generator import TDag\nfrom data_detective_airflow.operators import DBDump, PgSingleTargetLoader, PyTransform\nfrom data_detective_airflow.utils.petl_utils import appender_petl2pandas\n\nfrom common.builders import JsonSystemBuilder, TableInfoBuilder, TableInfoDescriptionType\nfrom common.urn import get_tree_node, get_schema, get_table, get_column\nfrom common.utils import get_readable_size_bytes\nfrom common.utilities.entity_enums import (\n    ENTITY_CORE_FIELDS, RELATION_CORE_FIELDS,\n    EntityTypes, EntityFields,\n    RelationTypes, RelationFields\n)\nfrom common.utilities.search_enums import CardType, SystemForSearch, TypeForSearch\n\n\ndef transform_schema_to_entity(_context: dict, schemas: DataFrame) -> tuple[tuple]:\n    \"\"\"Transform schema metadata to dds entity table format\n    :param _context: airflow DAG task run context\n    :param schemas: Dataframe['schema_name', 'schema_owner', 'schema_acl', 'schema_description']\n    :return: petl.tables(ENTITY_CORE_FIELDS + EntityFields.TABLES, EntityFields.JSON_SYSTEM, EntityFields.INFO)\n    \"\"\"\n    json_system_builder = JsonSystemBuilder(\n        system_for_search=SystemForSearch.POSTGRES.name,\n        type_for_search=TypeForSearch.SCHEMA.name,\n        card_type=CardType.SCHEMA.name,\n    )\n\n    schema_table_info_description = TableInfoDescriptionType(\n        keys={\n            'schema_owner': 'Owner',\n            'schema_acl': 'Access privileges',\n        },\n        header='General',\n        display_headers='0',\n        orientation='vertical'\n    )\n    schema_table_info_builder = TableInfoBuilder(schema_table_info_description)\n\n    result = (petl.fromdataframe(schemas)\n              .addfield(EntityFields.ENTITY_TYPE, EntityTypes.SCHEMA)\n              .addfield(EntityFields.ENTITY_NAME, lambda row: row['schema_name'])\n              .addfield(EntityFields.ENTITY_NAME_SHORT, None)\n              .addfield(EntityFields.URN,\n                        lambda row: get_schema('postgres', 'pg', 'airflow', row[EntityFields.ENTITY_NAME]))\n              .rename('schema_description', EntityFields.INFO)\n              .addfield(EntityFields.JSON_DATA,\n                        lambda row: dict(schema_owner=row['schema_owner'], schema_acl=row['schema_acl']))\n              .addfield(EntityFields.TABLES, lambda row: [schema_table_info_builder(row)])\n              .addfield(EntityFields.JSON_SYSTEM, json_system_builder())\n              .addfield(EntityFields.SEARCH_DATA,\n                        lambda row: f\"{row[EntityFields.URN]} {row[EntityFields.ENTITY_NAME]}\")\n              .cut(list(ENTITY_CORE_FIELDS)\n                   + [EntityFields.TABLES, EntityFields.JSON_SYSTEM, EntityFields.INFO])\n              .distinct(key=EntityFields.URN)\n              )\n    return result.tupleoftuples()\n\n\ndef transform_table_to_entity(_context: dict, tables: DataFrame) -> tuple[tuple]:\n    \"\"\"Transform tables metadata to dds entity table format\n    :param _context: airflow DAG task run context\n    :param tables: Dataframe['schema_name', 'table_name', 'table_owner', 'estimated_rows',\n                             'table_size', 'full_table_size', 'index_json']\n    :return: petl.tables(ENTITY_CORE_FIELDS + EntityFields.TABLES, EntityFields.JSON_SYSTEM)\n    \"\"\"\n    json_system_builder = JsonSystemBuilder(\n        system_for_search=SystemForSearch.POSTGRES.name,\n        type_for_search=TypeForSearch.TABLE.name,\n        card_type=CardType.TABLE.name,\n    )\n\n    table_size_description = TableInfoDescriptionType(\n        keys={\n            'table_owner': 'Owner',\n            'estimated_rows': 'Rows',\n            'table_size': 'Data size',\n            'full_table_size': 'Total relation size',\n        },\n        header='General',\n        display_headers='0',\n        orientation='vertical',\n        serializers={\n            'table_size': get_readable_size_bytes,\n            'full_table_size': get_readable_size_bytes,\n        },\n    )\n    table_size_builder = TableInfoBuilder(table_size_description)\n\n    table_index_description = TableInfoDescriptionType(\n        keys={\n            'name': 'Name',\n            'ddl': 'Definitions',\n        },\n        header='Table indexes',\n        display_headers='1',\n        orientation='horizontal'\n    )\n    table_index_builder = TableInfoBuilder(table_index_description)\n\n    table_rights_description = TableInfoDescriptionType(\n        keys={\n            'grantee': 'Account name',\n            'rights': 'Enable privilege',\n        },\n        header='Table rights',\n        display_headers='1',\n        orientation='horizontal'\n    )\n    table_rights_builder = TableInfoBuilder(table_rights_description)\n\n    result = (petl.fromdataframe(tables)\n              .addfield(EntityFields.URN,\n                        lambda row: get_table('postgres', 'pg', 'airflow', row['schema_name'], row['table_name']))\n              .addfield(EntityFields.ENTITY_NAME, lambda row: f\"{row['schema_name']}.{row['table_name']}\")\n              .rename('table_name', EntityFields.ENTITY_NAME_SHORT)\n              .addfield(EntityFields.ENTITY_TYPE, EntityTypes.TABLE)\n              .addfield(EntityFields.SEARCH_DATA,\n                        lambda row: f\"{row[EntityFields.URN]} {row[EntityFields.ENTITY_NAME]}\")\n              .addfield(EntityFields.JSON_SYSTEM, json_system_builder())\n              .addfield(EntityFields.JSON_DATA,\n                        lambda row: dict(estimated_rows=row['estimated_rows'],\n                                         table_size=row['table_size'],\n                                         full_table_size=row['full_table_size'],\n                                         index_json=row['index_json'],\n                                         table_rights=row['table_rights']))\n              .addfield(EntityFields.TABLES, lambda row: [table_size_builder(row),\n                                                          table_index_builder(row['index_json'] or {}),\n                                                          table_rights_builder(row['table_rights'] or {})])\n              .cut(list(ENTITY_CORE_FIELDS) + [EntityFields.TABLES, EntityFields.JSON_SYSTEM])\n              .distinct(key=EntityFields.URN)\n              )\n    return result.tupleoftuples()\n\n\ndef transform_column_to_entity(_context: dict, columns: DataFrame) -> tuple[tuple]:\n    \"\"\"Transform tables metadata to dds entity table format\n    :param _context: airflow DAG task run context\n    :param columns: Dataframe['schema_name', 'table_name', 'column_name', 'column_type', 'ordinal_position']\n    :return: petl.tables(ENTITY_CORE_FIELDS + EntityFields.TABLES, EntityFields.JSON_SYSTEM)\n    \"\"\"\n    json_system_builder = JsonSystemBuilder(\n        system_for_search=SystemForSearch.POSTGRES.name,\n        type_for_search=TypeForSearch.COLUMN.name,\n    )\n\n    column_table_info_description = TableInfoDescriptionType(\n        keys={\n            'column_type': 'Type',\n            'ordinal_position': 'Position',\n        },\n        header='General',\n        display_headers='0',\n        orientation='vertical'\n    )\n    column_table_info_builder = TableInfoBuilder(column_table_info_description)\n\n    result = (petl.fromdataframe(columns)\n              .addfield(EntityFields.URN,\n                        lambda row: get_column('postgres', 'pg', 'airflow',\n                                               row['schema_name'], row['table_name'], row['column_name']))\n              .addfield(EntityFields.ENTITY_NAME,\n                        lambda row: f\"{row['schema_name']}.{row['table_name']}.{row['column_name']}\")\n              .addfield(EntityFields.ENTITY_NAME_SHORT, lambda row: row['column_name'])\n              .addfield(EntityFields.ENTITY_TYPE, EntityTypes.COLUMN)\n              .addfield(EntityFields.SEARCH_DATA,\n                        lambda row: f\"{row[EntityFields.URN]} {row[EntityFields.ENTITY_NAME]}\")\n              .addfield(EntityFields.JSON_DATA,\n                        lambda row: dict(ordinal_position=row['ordinal_position'], column_type=row['column_type']))\n              .addfield(EntityFields.JSON_SYSTEM, json_system_builder())\n              .addfield(EntityFields.TABLES, lambda row: [column_table_info_builder(row)])\n              .cut(list(ENTITY_CORE_FIELDS) + [EntityFields.TABLES, EntityFields.JSON_SYSTEM])\n              .distinct(key=EntityFields.URN)\n              )\n    return result.tupleoftuples()\n\n\ndef link_schema_to_table(_context: dict, tables: DataFrame) -> tuple[tuple]:\n    \"\"\"Link schemas entity to tables\n    :param _context: airflow DAG task run context\n    :param tables: Dataframe['schema_name', 'table_name', 'table_owner', 'estimated_rows',\n                             'table_size', 'full_table_size', 'index_json']\n    :return: RELATION_CORE_FIELDS\n    \"\"\"\n    result = (petl.fromdataframe(tables)\n              .addfield(RelationFields.SOURCE,\n                        lambda row: get_schema('postgres', 'pg', 'airflow', row['schema_name']))\n              .addfield(RelationFields.DESTINATION,\n                        lambda row: get_table('postgres', 'pg', 'airflow', row['schema_name'], row['table_name']))\n              .addfield(RelationFields.TYPE, RelationTypes.Contains)\n              .addfield(RelationFields.ATTRIBUTE, None)\n              .cut(list(RELATION_CORE_FIELDS))\n              .distinct()\n              )\n    return result.tupleoftuples()\n\n\ndef link_table_to_column(_context: dict, columns: DataFrame) -> tuple[tuple]:\n    \"\"\"Link tables entity to columns\n    :param _context: airflow DAG task run context\n    :param columns: Dataframe['schema_name', 'table_name', 'column_name', 'column_type', 'ordinal_position']\n    :return: RELATION_CORE_FIELDS\n    \"\"\"\n    result = (petl.fromdataframe(columns)\n              .addfield(RelationFields.SOURCE,\n                        lambda row: get_table('postgres', 'pg', 'airflow', row['schema_name'], row['table_name']))\n              .addfield(RelationFields.DESTINATION,\n                        lambda row: get_column('postgres', 'pg', 'airflow',\n                                               row['schema_name'], row['table_name'], row['column_name']))\n              .addfield(RelationFields.TYPE, RelationTypes.Contains)\n              .addfield(RelationFields.ATTRIBUTE, None)\n              .cut(list(RELATION_CORE_FIELDS))\n              .distinct()\n              )\n    return result.tupleoftuples()\n\n\ndef link_root_node_to_schema(_context: dict, schemas: DataFrame) -> tuple[tuple]:\n    \"\"\"Link schemas to root tree node urn:tree_node:root:database\n    :param _context: airflow DAG task run context\n    :param schemas: Dataframe['schema_name', 'schema_owner', 'schema_acl', 'schema_description']\n    :return: RELATION_CORE_FIELDS\n    \"\"\"\n    result = (petl.fromdataframe(schemas)\n              .addfield(RelationFields.SOURCE, lambda row: get_tree_node(['Database']))\n              .addfield(RelationFields.DESTINATION,\n                        lambda row: get_schema('postgres', 'pg', 'airflow', row['schema_name']))\n              .addfield(RelationFields.TYPE, RelationTypes.Contains)\n              .addfield(RelationFields.ATTRIBUTE, None)\n              .cut(list(RELATION_CORE_FIELDS))\n              .distinct()\n              )\n    return result.tupleoftuples()\n\n\ndef fill_dag(t_dag: TDag):\n\n    DBDump(\n        task_id='dump_pg_schemas',\n        description='Dump schemas from pg database',\n        conn_id=PG_CONN_ID,\n        sql='\/code\/dump_schemas.sql',\n        dag=t_dag,\n    )\n\n    DBDump(\n        task_id='dump_pg_tables',\n        description='Dump tables from pg database',\n        conn_id=PG_CONN_ID,\n        sql='\/code\/dump_tables.sql',\n        dag=t_dag,\n    )\n\n    DBDump(\n        task_id='dump_pg_columns',\n        description='Dump columns from pg database',\n        conn_id=PG_CONN_ID,\n        sql='\/code\/dump_columns.sql',\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='transform_schema_to_entity',\n        description='Transform schemas metadata to dds.entity',\n        source=['dump_pg_schemas'],\n        transformer_callable=transform_schema_to_entity,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='transform_table_to_entity',\n        description='Transform tables metadata to dds.entity',\n        source=['dump_pg_tables'],\n        transformer_callable=transform_table_to_entity,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='transform_column_to_entity',\n        description='Transform columns metadata to dds.entity',\n        source=['dump_pg_columns'],\n        transformer_callable=transform_column_to_entity,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='link_schema_to_table',\n        description='Link schemas to tables for dds.relation',\n        source=['dump_pg_tables'],\n        transformer_callable=link_schema_to_table,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='link_table_to_column',\n        description='Link tables to columns for dds.relation',\n        source=['dump_pg_columns'],\n        transformer_callable=link_table_to_column,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='link_root_node_to_schema',\n        description='Link schemas to root tree node',\n        transformer_callable=link_root_node_to_schema,\n        source=['dump_pg_schemas'],\n        dag=t_dag\n    )\n\n    PyTransform(\n        task_id='append_entities',\n        description='Append entities to load in entity table',\n        source=['transform_schema_to_entity',\n                'transform_table_to_entity',\n                'transform_column_to_entity'],\n        transformer_callable=appender_petl2pandas,\n        dag=t_dag,\n    )\n\n    PyTransform(\n        task_id='append_relations',\n        description='Append relations to load in relation table',\n        source=['link_root_node_to_schema',\n                'link_schema_to_table',\n                'link_table_to_column'],\n        transformer_callable=appender_petl2pandas,\n        dag=t_dag,\n    )\n\n    PgSingleTargetLoader.upload_dds_entity(dag=t_dag, sources=['append_entities'])\n    PgSingleTargetLoader.upload_dds_relation(dag=t_dag, sources=['append_relations'])\n"
                }
            ],
            "schedule_interval":"13 03 * * *",
            "sql_files":[
                {
                    "name":"code\/dump_columns.sql",
                    "type":"sql",
                    "data":"with ns as (\n    select oid, nspname\n    from pg_namespace\n    where true\n      and nspname !~ '^pg_'\n      and nspname !~ '^wrk_dd_'\n      and nspname <> 'information_schema'\n)\nselect lower(ns.nspname)                               as schema_name,\n       lower(cl.relname)                               as table_name,\n       lower(f.attname)                                as column_name,\n       pg_catalog.format_type(f.atttypid, f.atttypmod) as column_type,\n       f.attnum                                        as ordinal_position\nfrom pg_attribute f\n         inner join pg_class cl\n                    on f.attrelid = cl.oid and cl.relkind in ('r', 'p')\n         inner join ns as ns\n                    on cl.relnamespace = ns.oid\nwhere true\n  and attnum > 0\n  and f.attisdropped is false\n;\n"
                },
                {
                    "name":"code\/dump_schemas.sql",
                    "type":"sql",
                    "data":"select lower(ns.nspname)                                  as schema_name,\n       pg_catalog.pg_get_userbyid(ns.nspowner)            as schema_owner,\n       pg_catalog.array_to_json(ns.nspacl)                as schema_acl,\n       pg_catalog.obj_description(ns.oid, 'pg_namespace') as schema_description\nfrom pg_namespace as ns\nwhere true\n  and ns.nspname !~ '^pg_'\n  and ns.nspname !~ '^wrk_dd_'\n  and ns.nspname <> 'information_schema'\n;\n"
                },
                {
                    "name":"code\/dump_tables.sql",
                    "type":"sql",
                    "data":"with ns as (\n    select oid, nspname\n    from pg_namespace\n    where true\n      and nspname !~ '^pg_'\n      and nspname !~ '^wrk_dd_'\n      and nspname <> 'information_schema'\n),\n     ind as (\n         select schemaname as schema_name,\n                tablename  as table_name,\n                jsonb_agg(\n                        jsonb_build_object(\n                                'name', indexname,\n                                'ddl', indexdef\n                            )\n                    )      as index_json\n         from pg_indexes\n         where True\n           and schemaname !~ '^pg_'\n           and schemaname !~ '^wrk_dd_'\n         group by schemaname, tablename\n     ),\n     table_grantee as (select grantee,\n                              table_schema,\n                              table_name,\n                              array_agg(privilege_type) as rights\n                       from information_schema.role_table_grants\n                       where true\n                         and table_schema !~ '^pg_'\n                         and table_schema !~ '^wrk_dd_'\n                         and table_schema <> 'information_schema'\n                       group by table_schema, table_name, grantee\n     ),\n     table_rights as (\n         select table_schema,\n                table_name,\n                jsonb_agg(jsonb_build_object(\n                        'grantee', grantee,\n                        'rights', rights)) as table_rights\n         from table_grantee\n         group by table_schema, table_name\n     )\nselect lower(ns.nspname)                        as schema_name,\n       lower(tbl.relname)                       as table_name,\n       pg_catalog.pg_get_userbyid(tbl.relowner) as table_owner,\n       coalesce(tbl.reltuples::bigint, 0)       as estimated_rows,\n       pg_table_size(tbl.oid)                   as table_size,\n       pg_total_relation_size(tbl.oid)          as full_table_size,\n       ind.index_json                           as index_json,\n       tr.table_rights                          as table_rights\nfrom pg_class as tbl\n         inner join ns as ns on ns.oid = tbl.relnamespace\n         left join ind as ind on ind.schema_name = ns.nspname and ind.table_name = tbl.relname\n         left join table_rights as tr on tr.table_schema = ns.nspname and tr.table_name = tbl.relname\nwhere tbl.relkind in ('r', 'p')\n;\n"
                }
            ],
            "tags":[
                "dds"
            ],
            "tasks":[
                "dump_pg_schemas",
                "dump_pg_tables",
                "dump_pg_columns",
                "transform_schema_to_entity",
                "transform_table_to_entity",
                "transform_column_to_entity",
                "link_schema_to_table",
                "link_table_to_column",
                "link_root_node_to_schema",
                "append_entities",
                "append_relations",
                "upload_dds_entity",
                "upload_dds_relation"
            ],
            "yaml_files":[

            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_dds_root",
            "dag_id":"dd_load_dds_root",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_type":"file"
            },
            "description":"Loading root entities",
            "factory":"YAML",
            "meta_yaml":"description: Loading root entities\nschedule_interval: '5 1 * * *'\ntags:\n  - tuning\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_type: file\nfactory: YAML\ntasks:\n\n  - task_id: dump_root_nodes_entities\n    description: Generate tree nodes\n    type: data_detective_airflow.operators.PythonDump\n    python_callable: dump_root_nodes_entities\n    op_kwargs:\n      file_name: root_nodes.yaml\n\n  - task_id: dump_root_nodes_relations\n    description: Create relations between tree nodes\n    type: data_detective_airflow.operators.PythonDump\n    python_callable: dump_root_nodes_relations\n    op_kwargs:\n      file_name: root_nodes.yaml\n\n  - description: Upload to dds.entity\n    type: data_detective_airflow.operators.upload_mg_entity\n    sources:\n    - dump_root_nodes_entities\n\n  - description: Upload to dds.relations\n    type: data_detective_airflow.operators.upload_mg_relation\n    sources:\n    - dump_root_nodes_relations\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"from io import StringIO\nfrom pathlib import Path\n\nimport yaml\nfrom pandas import DataFrame\n\nfrom common.urn import get_tree_node\nfrom common.utilities.entity_enums import EntityTypes, RelationTypes\n\n\ndef walk_relations(nodes: dict, source: tuple[str] = None) -> dict:\n    \"\"\"Observe root_nodes with outputting relationships\n    :param nodes: Nested hierarchical dictionary of tree_node\n    :param source: List of parents for tree_node\n    :return: Dict\n    \"\"\"\n    for key, value in nodes.items():\n        path = source + (key, ) if source and key != 'root' else (key, )\n        if 'contains' in value:\n            yield from walk_relations(value['contains'], path)\n        yield {'source': source, 'destination': path}\n\n\ndef walk_entities(nodes: dict, source: tuple[str] = None) -> dict:\n    \"\"\"Observe root_nodes with listing entities and their attributes\n    :param nodes: Nested hierarchical dictionary of tree_node\n    :param source: List of parents for tree_node\n    :return: Dict\n    \"\"\"\n    for key, value in nodes.items():\n        path = source + (key, ) if source and key != 'root' else (key, )\n        if 'contains' in value:\n            yield from walk_entities(value['contains'], path)\n        res = {'path': path}\n        json_data = {k: v for k, v in value.items() if k != 'contains'}\n        res.update({'json_data': json_data})\n        yield res\n\n\ndef dump_root_nodes_entities(context: dict, file_name: str) -> DataFrame:\n    \"\"\"Get entities for tree_node from root_nodes.yaml\n    :param context: Execution context\n    :param file_name: File name\n    :return: DataFrame\n    \"\"\"\n    raw = yaml.safe_load(StringIO(Path(f'{context[\"dag\"].etc_dir}\/{file_name}').read_text()))\n    root_nodes = DataFrame.from_dict(walk_entities(raw))\n    root_nodes['urn'] = root_nodes.apply(\n        lambda row: get_tree_node(row['path']),\n        axis=1\n    )\n    root_nodes['entity_name'] = root_nodes['path'].apply(lambda path: path[-1])\n    root_nodes['loaded_by'] = context['dag'].dag_id\n    root_nodes['entity_type'] = EntityTypes.TREE_NODE\n    root_nodes['entity_name_short'] = None\n    root_nodes['search_data'] = root_nodes['urn'] + ' ' + root_nodes['entity_name'].str.lower()\n    return root_nodes[['urn', 'entity_name', 'loaded_by', 'entity_type',\n                       'json_data', 'entity_name_short', 'search_data']]\n\n\ndef dump_root_nodes_relations(context: dict, file_name: str) -> DataFrame:\n    \"\"\"Get relations between tree_node from root_nodes.yaml\n    :param context: Execution context\n    :param file_name: File name\n    :return: DataFrame\n    \"\"\"\n    raw = yaml.safe_load(StringIO(Path(f'{context[\"dag\"].etc_dir}\/{file_name}').read_text()))\n    root_nodes = DataFrame.from_dict(walk_relations(raw))\n\n    root_nodes = root_nodes[~root_nodes['source'].isnull()]\n\n    root_nodes['source'] = root_nodes.apply(\n        lambda row: get_tree_node(row['source']),\n        axis=1\n    )\n    root_nodes['destination'] = root_nodes.apply(\n        lambda row: get_tree_node(row['destination']),\n        axis=1\n    )\n    root_nodes['type'] = RelationTypes.Contains\n    root_nodes['loaded_by'] = context['dag'].dag_id\n    root_nodes['attribute'] = None\n\n    return root_nodes[['source', 'destination', 'type', 'loaded_by', 'attribute']]\n"
                }
            ],
            "schedule_interval":"5 1 * * *",
            "sql_files":[

            ],
            "tags":[
                "tuning"
            ],
            "tasks":[
                "dump_root_nodes_entities",
                "dump_root_nodes_relations",
                "upload_dds_entity",
                "upload_dds_relation"
            ],
            "yaml_files":[
                {
                    "name":"etc\/root_nodes.yaml",
                    "type":"yaml",
                    "data":"root:\n  info: The root entity\n  contains:\n\n    Documentation:\n      info: >\n        Service documentation\n\n    Database:\n      info: >\n        Metadata of schemas, tables and columns in Data Detective postgres database\n\n    ETL DAGS:\n      info: >\n        Metadata of Directed Acyclic Graph loaded metadata to Data Detective database\n\n    Data Detective:\n      info: >\n        This section contains all available information about Data Detective\n      contains:\n\n        Logical Model:\n          info: >\n            Description of tables in the Data Detective service\n\n        Physical Model:\n          info: This section contains technical information on Data Detective tables\n\n        DAGs:\n          info: This section contains information on all ETL processes in the Data Platform\n"
                }
            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_tuning_breadcrumb",
            "dag_id":"dd_load_tuning_breadcrumb",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_type":"file"
            },
            "description":"Loading the breadcrumb of relations from root to entity",
            "factory":"YAML",
            "meta_yaml":"description: Loading the breadcrumb of relations from root to entity\nschedule_interval: 0 *\/2 * * *\ntags:\n  - tuning\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_type: file\nfactory: YAML\ntasks:\n\n  - task_id: dump_relation_contains\n    description: Load relations with type Contains\n    type: data_detective_airflow.operators.DBDump\n    sql: \/code\/dump_relation_contains.sql\n    conn_id: pg\n\n  - task_id: transform_breadcrumb\n    description: Create transform breadcrumbs\n    type: data_detective_airflow.operators.PyTransform\n    source:\n      - dump_relation_contains\n    transformer_callable: transform_breadcrumb\n\n  - task_id: upload_tuning_breadcrumb\n    description: Upload entities to tuning.breadcrumb\n    type: data_detective_airflow.operators.PgSingleTargetLoader\n    chunk_row_number: 30000\n    source:\n      - transform_breadcrumb\n    conn_id: pg\n    table_name: tuning.breadcrumb\n    key:\n      - urn\n    filter_callable: data_detective_airflow.operators.sinks.pg_single_target_utils.filter_for_breadcrumb\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"import json\n\nfrom pandas import DataFrame, concat\n\n\ndef transform_breadcrumb(_context: dict, df: DataFrame) -> DataFrame:\n    \"\"\"Create transform breadcrumbs\n    :param _context: Execution context\n    :param df: ['destination', 'source', 'entity_name']\n    :return: DataFrame ['urn', 'breadcrumb_urn', 'breadcrumb_entity']\n    \"\"\"\n    root = DataFrame()\n    root['urn'] = df[df['source'] == 'urn:tree_node:root']['destination']\n    root['breadcrumb_urn'] = root['urn'].apply(lambda urn: list())\n    root['breadcrumb_entity'] = root['urn'].apply(lambda urn: list())\n\n    result = [root]\n    layer_ind = 0\n    while not result[layer_ind].empty:\n        layer = result[layer_ind].merge(df, left_on='urn', right_on='source')\n        layer['urn'] = layer['destination']\n        layer['breadcrumb_urn'] = layer['breadcrumb_urn'] + layer['source'].apply(lambda urn: [urn])\n        layer['breadcrumb_entity'] = layer['breadcrumb_entity'] + layer['entity_name'].apply(lambda urn: [urn])\n        result.append(layer[['urn', 'breadcrumb_urn', 'breadcrumb_entity']])\n        layer_ind += 1\n\n    result = concat(result, ignore_index=True)\n    result['breadcrumb_urn'] = result['breadcrumb_urn'].apply(json.dumps, ensure_ascii=False)\n    result['breadcrumb_entity'] = result['breadcrumb_entity'].apply(json.dumps, ensure_ascii=False)\n\n    return result[['urn', 'breadcrumb_urn', 'breadcrumb_entity']].drop_duplicates(subset='urn')\n"
                }
            ],
            "schedule_interval":"0 *\/2 * * *",
            "sql_files":[
                {
                    "name":"code\/dump_relation_contains.sql",
                    "type":"sql",
                    "data":"SELECT r.destination                                as destination,\n       r.source                                     as source,\n       coalesce(e.entity_name_short, e.entity_name) as entity_name\nFROM dds.relation r\n         LEFT JOIN dds.entity e ON r.source = e.urn\nWHERE r.type = 'Contains';\n"
                }
            ],
            "tags":[
                "tuning"
            ],
            "tasks":[
                "dump_relation_contains",
                "transform_breadcrumb",
                "upload_tuning_breadcrumb"
            ],
            "yaml_files":[

            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_tuning_relations_type",
            "dag_id":"dd_load_tuning_relations_type",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_conn_id":"s3work",
                "work_type":"s3"
            },
            "description":"Loading entity relationship types",
            "factory":"YAML",
            "meta_yaml":"# Loading entity relationship types\n\ndescription: Loading entity relationship types\nschedule_interval: '5 2 * * *'\ntags:\n  - SLA3\n  - prod_dev\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_conn_id: s3work\n  work_type: s3\nfactory: YAML\ntasks:\n\n  - task_id: dump_relations_types\n    description: Load relations types\n    type: data_detective_airflow.operators.PythonDump\n    python_callable: dump_relations_types\n    op_kwargs:\n      file_name: relations_type.md\n\n  - task_id: upload_tuning_relations_type\n    description: Upload to relations_type\n    type: data_detective_airflow.operators.PgSCD1\n    source:\n    - dump_relations_types\n    conn_id: pg\n    table_name: tuning.relations_type\n    process_deletions: true\n    key:\n      - source_type\n      - target_type\n      - relation_type\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"import pandas\n\n\ndef dump_relations_types(context: dict, file_name: str) -> pandas.DataFrame:\n    \"\"\"Get entities for tree_node from root_nodes.yaml\n    :param context: Execution context\n    :param file_name: File name\n    :return: DataFrame ['source_type', 'target_type', 'attribute_type', 'relation_type',\n                        'source_group_name', 'target_group_name', 'attribute_group_name',\n                        'loaded_by']\n    \"\"\"\n\n    data = pandas.read_csv(f\"{context['dag'].etc_dir}\/{file_name}\", sep='|', skiprows=[1])\n\n    for col in data.columns.tolist():\n        data[col] = data[col].str.strip()\n\n    data = data[(data['source_group_name'].str.len() > 0)\n                | (data['target_group_name'].str.len() > 0)\n                | (data['attribute_group_name'].str.len() > 0)]\n    data['loaded_by'] = context['dag'].dag_id\n\n    return data[['source_type', 'target_type', 'attribute_type', 'relation_type',\n                 'source_group_name', 'target_group_name', 'attribute_group_name',\n                 'loaded_by']]\n"
                }
            ],
            "schedule_interval":"5 2 * * *",
            "sql_files":[

            ],
            "tags":[
                "SLA3",
                "prod_dev"
            ],
            "tasks":[
                "dump_relations_types",
                "upload_tuning_relations_type"
            ],
            "yaml_files":[

            ]
        },
        {
            "dag_dir":"\/usr\/local\/airflow\/dags\/dags\/dd_load_tuning_search_help",
            "dag_id":"dd_load_tuning_search_help",
            "default_args":{
                "owner":"airflow",
                "result_type":"pickle",
                "retries":1,
                "work_conn_id":"s3work",
                "work_type":"s3"
            },
            "description":"Loading information about systems and types into tuning.search_help table",
            "factory":"Python",
            "meta_yaml":"description: Loading information about systems and types into tuning.search_help table\nschedule_interval: 12 03 * * *\ntags:\n  - tuning\ndefault_args:\n  owner: airflow\n  result_type: pickle\n  retries: 1\n  work_conn_id: s3work\n  work_type: s3\nfactory: Python\n",
            "py_files":[
                {
                    "name":"code\/code.py",
                    "type":"py",
                    "data":"from pandas import DataFrame\n\nfrom data_detective_airflow.constants import PG_CONN_ID\nfrom data_detective_airflow.dag_generator import TDag\nfrom data_detective_airflow.operators import PgSCD1, PyTransform\n\nfrom common.utilities.search_enums import SystemForSearch, TypeForSearch\n\n\ndef get_data_from_search_enums(context: dict) -> DataFrame:\n    \"\"\"Get data from SystemForSearch and TypeForSearch\n    :param context: Execution context\n    :returns: Dataframe ['type', 'key', 'name', 'description']\n    \"\"\"\n    search_help_fields = ['type', 'name', 'description', 'loaded_by']\n    systems = [['SYSTEM',\n                search_system.name,\n                search_system.description,\n                context['dag'].dag_id]\n               for _, search_system in SystemForSearch.__members__.items()]\n    types = [['TYPE',\n              search_type.name,\n              search_type.description,\n              context['dag'].dag_id]\n             for _, search_type in TypeForSearch.__members__.items()]\n\n    return DataFrame(systems + types, columns=search_help_fields)\n\n\ndef fill_dag(t_dag: TDag):\n\n    PyTransform(\n        task_id='get_data_from_search_enums',\n        description='Get data from SystemForSearch and TypeForSearch classes',\n        transformer_callable=get_data_from_search_enums,\n        dag=t_dag,\n    )\n\n    PgSCD1(\n        task_id='upload_data_to_tuning_search_help',\n        description='Upload data into tuning.search_help table',\n        conn_id=PG_CONN_ID,\n        process_deletions=True,\n        table_name='tuning.search_help',\n        source=['get_data_from_search_enums'],\n        key=['type', 'name'],\n        dag=t_dag,\n    )\n"
                }
            ],
            "schedule_interval":"12 03 * * *",
            "sql_files":[

            ],
            "tags":[
                "tuning"
            ],
            "tasks":[
                "get_data_from_search_enums",
                "upload_data_to_tuning_search_help"
            ],
            "yaml_files":[

            ]
        }
    ]
}