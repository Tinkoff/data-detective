version: '2.3'

x-airflow: &airflow
  build:
    context: .
    target: dev
  environment:
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${META_USER}:${META_PASS}@metadb:5432/${META_USER}
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__WEBSERVER__SECRET_KEY=${SECRET_KEY}
    - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
    - AIRFLOW__CORE__LOAD_EXAMPLES=False

services:
  ssh_service:
    build:
      context: docker/ssh_service

  metadb:
    image: postgres:13.4-alpine
    environment:
      - POSTGRES_USER=${META_USER}
      - POSTGRES_PASSWORD=${META_PASS}
    ports:
      - "5004:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 2s
      timeout: 5s
      retries: 10

  pg:
    image: postgres:13.4-alpine
    environment:
      - POSTGRES_USER=${META_USER}
      - POSTGRES_PASSWORD=${META_PASS}
    ports:
      - "5005:5432"
    volumes:
      - ./docker/init-pgwork.sql:/docker-entrypoint-initdb.d/init-pgwork.sql

  s3:
    image: localstack/localstack:0.11.5
    ports:
      - "4566:4566"
      - "8055:8080"
    environment:
      - SERVICES=s3
      - DEBUG=0
      - DATA_DIR=/tmp/localstack/data
    volumes:
      - ./docker/init-s3work.sh:/docker-entrypoint-initaws.d/init-s3work.sh

  app:
    <<: *airflow
    depends_on:
      metadb:
        condition: service_healthy
    ports:
      - "127.0.0.1:9922:22"
      - "8080:8080"
    volumes:
      - ./dags:${AIRFLOW_HOME}/dags
      - ./data_detective_airflow:${AIRFLOW_HOME}/data_detective_airflow
      - ./tests:${AIRFLOW_HOME}/tests
      - ./tests_data:${AIRFLOW_HOME}/tests_data
      - ./Makefile:${AIRFLOW_HOME}/Makefile
      - ./pyproject.toml:${AIRFLOW_HOME}/pyproject.toml
      - ./setup.cfg:${AIRFLOW_HOME}/setup.cfg
    command: all-in-one
