schedule_interval: '*/5 * * * *'
default_args:
  owner: airflow
  retries: 1
  result_type: pickle
  work_type: file
factory: YAML
tasks:
- task_id: df_first
  type: mg_airflow.operators.extractors.DBDump
  conn_id: pg
  sql: select now() as value;
- task_id: df_second
  type: mg_airflow.operators.extractors.DBDump
  conn_id: pg
  sql: select now() as value;
- task_id: df_third
  type: mg_airflow.operators.extractors.DBDump
  conn_id: pg
  sql: select now() as value;
- source:
  - df_first
  - df_second
  - df_third
  task_id: append_all
  type: mg_airflow.operators.transformers.PyTransform
  transformer_callable: >
    lambda _context, df_now1, transform: df_first.append(df_second, sort=False).append(df_third, sort=False)

