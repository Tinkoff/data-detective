schedule_interval: '*/5 * * * *'
default_args:
  owner: airflow
  retries: 1
  result_type: pickle
  work_type: file
factory: YAML
tasks:
- task_id: get_df
  type: mg_airflow.operators.extractors.db_dump.DBDump
  conn_id: pg
  sql: select now() as value;
- task_id: test
  type: mg_airflow.operators.transformers.PyTransform
  source:
  - get_df
  transformer_callable: 'lambda _context, df: df[["value"]]'
